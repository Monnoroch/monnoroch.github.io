---
layout: post
title: Генеративное Моделирование и AI
date: '2018-02-05 12:20:00 +0300'
categories: ru posts
---
В [предыдущей главе](/ru/posts/2017/10/30/generative-modeling-with-deep-learning.html) мы поговорили о классических дискриминативных моделях в машинном обучении и разобрали простейшие примеры таких моделей. Давайте теперь посмотрим на более общую картину.

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/learning_process.gif" width=520 %} </center>

## Задача Искусственного Интеллекта

Искусственный Интеллект (ИИ) &mdash; это алгоритм, умеющий решать задачи, типично решаемые людьми, на уровне сопоставимом или превосходящем человеческий. Это может быть распознавание визуальных образов, понимание и анализ текстов, управление механизмами и построение логических цепочек. Система искусственного интеллекта &mdash; это система, которая сможет эффективно решать сразу все подобные задачи. Решения этой задачи до сих пор не существует, но есть разнообразные подходы, которые можно назвать первыми шагами в направлении решения проблемы ИИ.

В двадцатом веке самым популярным был подход, основанный на правилах. Его идея заключается в том, что мир подчиняется законам, которые, например, изучает физика и другие естественные науки. И даже если их нельзя эффективно запрограммировать в ИИ напрямую, разумно предположить, что при достаточном числе запрограммированных правил, ИИ система, основанная на вычислительных машинах, сможет эффективно существовать в мире, основанном на этих правилах, и решать произвольные задачи. Этот подход не учитывает естественную стохастичность некоторых событий. Чтобы ее учесть, необходимо на этом списке правил построить вероятностную модель для принятия стохастических решений при условии случайных входных данных.

Полное описание мира потребовало бы настолько большого числа правил, что их невозможно задать и поддерживать вручную. Отсюда возникает идея провести достаточно наблюдений событий в реальном мире и вывести необходимые правила автоматически из этих наблюдений. Эта идея сразу же поддерживает как действительную стохастичность некоторых естественных процессов, так дополнительно и мнимую стохастичность, возникающую из-за того, что не до конца известно, какие факторы влияют на некий детерминированный процесс. Автоматическим выводом правил из наблюдений занимается раздел математики, называемый машинным обучением, и именно оно в данный момент является наиболее обещающим фундаментом для ИИ.

## Машинно обученный ИИ

В предыдущей главе мы посмотрели на классические задачи машинного обучения (классификация и регрессия) и классические линейные методы их решения (логистическая и линейная регрессии). В реальности для сложных задач используют нелинейные модели, обычно основанные либо на решающих деревьях, либо на глубоких искусственных нейронных сетях, которые лучше себя показывают для задач обработки плотных данных с высоким уровнем избыточности: изображений, звука и текста.

Эти методы способны автоматически выводить правила на основе наблюдений, и они очень успешно применяются в коммерческих приложениях. Однако, у них есть недостаток, из-за которого они недостаточны для использования в качестве ИИ-системы &mdash; они специально разработаны для решения конкретной узкоспециализированной задачи, например, различать кошек и собак на изображении. Очевидно, что модель, суммирующая изображение в одно число, теряет много данных. Чтобы определить на изображении кошку не обязательно понимать суть кошки, достаточно лишь научиться искать основные признаки этой кошки. Задача классификации изображения не предполагает полного понимания сцены, а только поиск в ней конкретных объектов. Определить классификатор для всех возможных комбинаций связей всех возможных комбинаций объектов не представляется возможным хотя бы из-за экспоненциально большого объема наблюдений, которые нужно собрать и разметить. Потому идеи классических задач машинного обучения с подкреплением, в которых решается конкретная поставленная задача, не подходят. Для ИИ нужен принципиально иной подход к постановке задачи для машинного обучения.

## Вероятностная постановка задачи понимания мира

Итак, нам нужно, имея набор наблюдений, в каком-то смысле понять процесс, их порождающий. Переформулируем эту задачу на вероятностном языке. Пусть наблюдение &mdash; это реализация случайной величины $x: \Omega \to X$, $x \sim P(x)$, и есть набор наблюдаемых событий $D = \{x_i \sim P(x), i=\overline{1,N}\}$. Тогда “понимание” всего разнообразия событий можно сформулировать как задачу восстановления распределения $P(x)$.

Есть несколько подходов к решению этой задачи. Один из самых общих методов &mdash; введение латентной переменной. Допустим, существует некое представления $z: \Omega \to Z$, $z \sim P(z)$ наблюдения $x$. Это представление описывает “понимание” наблюдения моделью. Например, для кадра компьютерной игры таким пониманием может служить релевантное состояние игрового мира и положение камеры. В таком случае $P(x) = \int_Z P(x|z)P(z)dz$. Фиксируя $P(z)$ так, чтобы оно было простым для сэмплирования распределением, мы получаем модель, в которой $P(x|z)$ и $P(z|x)$ можно приближать нейросетями. Обучая эту модель стандартными методами глубокого обучения можно получить $P(x)$ по формуле выше, после чего мы можем использовать ее в вероятностном выводе. Более точные формулировки таких моделей будут в последующих частях, но тут нужно заметить, что сложные версии таких моделей требуют аппроксимации сложных невычислимых интегралов, для чего обычно используются приближенные методы вроде [MCMC](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) или [Variational Inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). Построение $P(x)$ для извлечения из нее сэмплов называют задачей генеративного моделирования.

Можно подойти к вопросу иначе. На самом деле, в явном виде рассчитывать $P(x)$, аппроксимируя сложные интегралы, не обязательно. Одна из идей состоит в том, что если модель может “вообразить” себе мир, значит, она понимает, как он устроен. Например, если я могу нарисовать человека в разных позах и ракурсах, значит я понимаю, как устроена его анатомия и перспектива в целом. Если люди не могут отличить сгенерированные моделью объекты от настоящих, значит модель смогла честно “понять”, как работает то или иное явление настолько же хорошо, насколько его понимают эти люди. Эта идея вдохновляет развитие генеративного моделирования с неявной $P(x)$ &mdash; разработке моделей, которые, имея конечное число наблюдений, способны обобщать их и генерировать новые наблюдения, неотличимые от настоящих. Предположим, что $z \sim N(0, 1)$, или любое другое простое для сэмплирования распределение. Тогда при достаточно общих условиях существует функция $f: Z \to X$, такая, что $f(z) \sim P(x)$. В этом случае мы не получаем $P(x)$ в явном виде, но модель все равно содержит информацию о нем неявно. Вместо восстановления $P(x)$ можно восстановить $f(z)$, после чего сэмплы из $P(x)$ могут быть получены как $f(z), z \sim N(0,1)$. $f(z)$ нельзя использовать в вероятностном выводе напрямую, но, во-первых, это не всегда нужно, а во-вторых, когда это нужно, часто можно воспользоваться методами Монте-Карло, для которых как раз и нужно получать сэмплы. К методам этого типа относится и модель Generative Adversarial Networks, исследуемая в следующей части.

## Principal Component Analysis

Давайте посмотрим на простую генеративную модель. Пусть есть некая наблюдаемая величина $x \sim P(x)$. Например это может быть рост человека или пиксельное изображение. Предположим, что эта величина полностью объясняется некой скрытой (латентной) величиной $z \sim P(z)$. В нашей аналогии это может быть вес человека или объект и его ориентация на изображении.  Предположим теперь, что для скрытой величины $P(z) = N(z; 0, 1)$, а наблюдаемое величина $x$ линейно зависит от $z$ с нормальным шумом, т.е. $P(x|z) = N(x; Wz + b, \sigma^2 I)$. Эта модель называется Probabilistic Principal Component Analysis (PPCA) и она, по сути, является вероятностной переформулировкой классической модели [Principal Component Analysis (PCA)](https://en.wikipedia.org/wiki/Principal_component_analysis), в которой видимая переменная $x$ линейно зависит от латентной переменной $z$ без шума.

## Expectation Maximization

[Expectation Maximization (EM)](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) &mdash; алгоритм для обучения моделей с латентными переменными. Детали можно найти в специализированной литературе, но общая суть алгоритма довольно проста:

1. Инициализировать модель начальными значениями параметров.
2. E-шаг. Заполнить латентные переменные их ожидаемыми значениями в текущей модели.
3. M-шаг. Максимизировать правдоподобие модели с фиксированными значениями латентных переменных. Например, градиентным спуском по параметрам.
4. Повторять (2, 3), пока ожидаемые значения латентных переменных не перестанут изменяться.

Если на М-шаге не максимизировать правдоподобие до конца, а только делать шаг в направлении максимума, это называется Generalized EM (GEM).

## Решение PCA с помощью EM

Применим к нашей модели PCA EM алгоритм и метод максимума правдоподобия для поиска оптимальных параметров $\theta = (W, b, \sigma)$ модели. Совместное правдоподобие наблюдаемых и латентных параметров можно записать как:

\\begin{equation}
    L(x|\theta)=\log P(x|\theta)=\log P(x|\theta) \int\_z q(z)=\int\_z q(z) \\log P(x|\theta)
\\end{equation}

Где $q(z)$ &mdash; это произвольное распределение. Далее будем опускать условность распределений по параметрам для облегчения формул.

\\begin{equation}
    \int\_z q(z) \log P(x|\theta)=\int\_z q(z) \log \\frac{P(x, z)}{P(z|x)}=\int\_z q(z) \log \frac{P(x, z)q(z)}{q(z)P(z|x)}=
\\end{equation}
\\begin{equation}
    =\int\_z q(z) \log P(x, z)-\int\_z q(z) \log q(z)+\int\_z q(z) \log \\frac{q(z)}{P(z|x)}=
\\end{equation}
\\begin{equation}
    =\int\_z q(z) \log P(x, z)+H\left(q\left(z\right)\right)+KL(q(z)||P(z|x))
\\end{equation}

Где величина $KL(q(z)||P(z|x))=\int\_z q(z) \log \frac{q(z)}{P(z|x)}$ называется $KL$-дивергенцией между распределениями $q(z)$ и $P(z|x)$. Величина $H\left(q\left(z\right)\right)=-\int\_z q(z) \log q(z)$ называется энтропией $q(z)$. $H\left(q\left(z\right)\right)$ не зависит от параметров $\theta$, поэтому это слагаемое мы можем игнорировать при оптимизации:

\\begin{equation}
    L(x|\theta) \propto \int\_z q(z) \log P(x, z)+KL(q(z)||P(z|x))=
\\end{equation}
\\begin{equation}
    =\int\_z q(z) \log \left( P(x|z)P(z)\right)+KL(q(z)||P(z|x))=
\\end{equation}
\\begin{equation}
    =\int\_z q(z) \log P(x|z)+\int\_z q(z) \log P(z)+KL(q(z)||P(z|x)) \propto
\\end{equation}
\\begin{equation}
    \propto \int\_z q(z) \log P(x|z)+KL(q(z)||P(z|x)).
\\end{equation}

Величина $KL(q(z)||P(z|x))$ неотрицательна и равна нулю когда $q(z)=P(z|x)$. В связи с этим давайте определим следующий EM алгоритм:

1. E: $q(z) := P(z|x)$. Это обнулит второе слагаемое $KL(q(z)||P(z|x))$.
2. M: Максимизация первого слагаемого $L(x|\theta) \propto \int_z q(z) \log P(x|z)$.

PPCA &mdash; линейная модель, потому ее можно решить аналитически. Но вместо этого мы попробуем решить ее с помощью обобщенного EM-алгоритма, когда максимизация выполняется не до конца, а только одним шагом градиентного подъема в сторону оптимума. Более того, мы будем использовать стохастический градиентный подъем, т.е. его шаг будет шагом в сторону оптимума только в среднем. Так как наши данные i.i.d., то

\\begin{equation}
    L(x|\theta) \propto  \int\_z q(z) \log P(x|z)=\int\_z q(z) \log \prod_{i=1}^{N} P(x\_i|z\_i)=\int\_z q(z) \sum\_{i=1}^{N} \log P(x\_i|z\_i)
\\end{equation}

Заметим, что выражение вида $\int\_zq(z)f(z)$ является математическим ожиданием $E\_{z \sim q(z)} f(z)$. Тогда

\\begin{equation}
    \int\_z q(z) \sum\_{i=1}^{N} \log P(x\_i|z\_i)=E\_{z \sim q(z)} \sum\_{i=1}^{N} \log P(x\_i|z\_i) \propto E\_{z \sim q(z)} \frac{1}{N}\sum\_{i=1}^{N} \log P(x\_i|z\_i)
\\end{equation}

Так как единичный сэмпл является несмещенной оценкой математического ожидания, то можно приближенно записать следующее равенство:

\\begin{equation}
    E\_{z \sim q(z)} \frac{1}{N}\sum\_{i=1}^{N} \log P(x\_i|z\_i)=\frac{1}{N} \sum\_{i=1}^{N} \log P(x\_i|z\_i).
\\end{equation}

Итого, подставляя $P(x|z) = N(x; Wz + b, \sigma^2 I)$, получим:

\\begin{equation}
    L(x|\theta) \propto \frac{1}{N} \sum\_{i=1}^{N} \log P(x\_i|z\_i)=
\\end{equation}
\\begin{equation}
    = \frac{1}{N} \sum\_{i=1}^{N} \log\left(\frac{1}{\sqrt{\left(2 \pi \right)^d \left| \sigma^2 I\right|}}  \exp\left(-\frac{||x\_i - \left(Wz\_i + b\right)||^2}{2 \sigma^2}\right)\right)=
\\end{equation}
\\begin{equation}
    = \frac{1}{N} \sum_{i=1}^{N} \log\left(\frac{1}{\sqrt{\left(2 \pi \sigma^2\right)^d}} \exp\left(-\frac{||x\_i - \left(Wz\_i + b\right)||^2}{2 \sigma^2}\right)\right)=
\\end{equation}
\\begin{equation}
    = \frac{1}{N} \sum\_{i=1}^{N} \left( - \log \sqrt{\left(2 \pi \sigma^2\right)^d} - \frac{1}{2 \sigma^2}{||x\_i - b - W z\_i||}^2\right)
\\end{equation}
или
\\begin{equation}
    L(x|\theta) \propto L^{*}(x|\theta)=-\frac{1}{N} \sum\_{i=1}^{N} \left(d\log\left(\sigma^2\right) + \frac{1}{\sigma^2}{||x\_i - b - W z\_i||}^2\right)
\\end{equation}
<center>Формула 1. Функция потерь, пропорциональная правдоподобию данных в модели PPCA.</center>

Где $d$ &mdash; размерность наблюдаемой переменной $x$. Теперь выпишем GEM-алгоритм для PPCA. $P(x|z) = N(x; Wz + b, \sigma^2 I)$, а $P(z|x)=N\left(z; \left(W^T W + \sigma^2 I \right)^{-1} W^T\left(x - b\right), \sigma^2 \left(W^T W + \sigma^2 I \right)^{-1} \right)$. Тогда GEM-алгоритм будет выглядеть так:

1. Инициализируем параметры $W, b, \sigma$ разумными случайными начальными приближениями.
2. Сэмплируем ${x_i} \sim P(x)$ &mdash; выборка минибатча из данных.
3. Рассчитываем ожидаемые значения латентных переменных $z\_i \sim P(z|x\_i)$ или $z\_i = \left(W^T W + \sigma^2 I \right)^{-1} W^T\left(x\_i - b\right) + \varepsilon, \varepsilon \sim N(0, \sigma^2 \left(W^T W + \sigma^2 I \right)^{-1})$.
4. Подставляем $x\_i, z\_i$ в формулу (1) для $L^{*}(x|\theta)$ и делаем шаг градиентного подъема по параметрам $W, b, \sigma$. Важно помнить, что $z\_i$ надо воспринимать как входы, и не пускать обратное распространение ошибки внутрь него.
5. Если правдоподобие данных и ожидаемые значения латентных переменных для контрольных видимых переменных не сильно изменяются, останавливаем обучение. Иначе, идем на шаг (2).

После того, как модель обучена, из нее можно генерировать сэмплы:
\\begin{equation}
    P(x)=N(x; b, W W^T + \sigma^2 I)
\\end{equation}

## Численное решение задачи PCA

Давайте обучим модель PPCA с помощью стандартного SGD. Мы опять будем изучать работу модели на игрушечном примере, чтобы понять все детали. Полный код модели можно найти [тут](https://github.com/Monnoroch/generative/tree/master/ppca), а в этой статье будут освещены лишь ключевые моменты.

Положим $P(x)=N(x;\\begin{pmatrix} 5 \\\\ 10 \\end{pmatrix}, \\begin{pmatrix} 1.2^2 & 0 \\\\ 0 & 2.4^2 \\end{pmatrix})$ &mdash; двумерное нормальное распределение с диагональной ковариационной матрицей. $P(z)=N(z; 0, 1)$ &mdash; одномерное нормальное латентное представление.

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/ellipse_around_average.png" description="Рис. 1. Эллипс вокруг среднего, в который попадает 95% точек из распределения $P(x)$." width=520 %} </center>

Итак, первое, что нужно сделать &mdash; это сгенерировать данные. Мы генерируем сэмплы из $P(x)$:

```python
def normal_samples(batch_size):
  def example():
    return tf.contrib.distributions.MultivariateNormalDiag(
     [5, 10], [1.2, 2.4]).sample(sample_shape=[1])[0]
  return tf.contrib.data.Dataset.from_tensors([0.])
    .repeat()
    .map(lambda x: example())
    .batch(batch_size)
```

Теперь нужно определить параметры модели:

```python
input_size = 2
latent_space_size = 1
stddev = tf.get_variable(
  "stddev", initializer=tf.constant(0.1, shape=[1]))
biases = tf.get_variable(
  "biases", initializer=tf.constant(0.1, shape=[input_size]))
weights = tf.get_variable(
  "Weights",
   initializer=tf.truncated_normal(
     [input_size, latent_space_size], stddev=0.1))
```

После этого можно получить для сэмпла видимых переменных их латентные представления:

```python
def get_latent(visible, latent_space_size, batch_size):
  matrix = tf.matrix_inverse(
    tf.matmul(weights, weights, transpose_a=True)
      + stddev**2 * tf.eye(latent_space_size))
  mean_matrix = tf.matmul(matrix, weights, transpose_b=True)
  # Multiply each vector in a batch by a matrix.
  expected_latent = batch_matmul(
    mean_matrix, visible - biases, batch_size)
  stddev_matrix = stddev**2 * matrix
  noise =
    tf.contrib.distributions.MultivariateNormalFullCovariance(
      tf.zeros(latent_space_size),
      stddev_matrix)
        .sample(sample_shape=[batch_size])
  return tf.stop_gradient(expected_latent + noise)
```

Тут нужно обратить внимание на tf.stop_gradient(...). Эта функция не дает значениям параметров внутри входного подграфа влиять на градиент по этим параметрам. Это нужно, чтобы $q(z) := P(z|x)$ оставалось фиксированным в течении M-шага, что необходимо для корректной работы EM-алгоритма.

Давайте теперь запишем функцию потерь $L^{*}(x|\theta)$ для оптимизации на M-шаге:

```python
sample = dataset.get_next()
latent_sample = get_latent(sample, latent_space_size, batch_size)
norm_squared = tf.reduce_sum((sample - biases -
  batch_matmul(weights, latent_sample, batch_size))**2, axis=1)
loss = tf.reduce_mean(
  input_size * tf.log(stddev**2) + 1/stddev**2 * norm_squared)
train = tf.train.AdamOptimizer(learning_rate)
  .minimize(loss, var_list=[bias, weights, stddev], name="train")
```

Итак, модель готова для тренировки. Давайте взглянем на кривую обучения модели:

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/learning_curve.png" description="Рис. 2. Кривая обучения модели PPCA." width=520 %} </center>

Видно, что модель достаточно регулярно и быстро сходится, что хорошо отражает простоту модели. Давайте посмотрим на выученные параметры распределения.

<center><img src="/images/posts/2018-02-05-generative-modeling-and-ai/displacement_graphs_b_0.png" height=256 alt="Ошибка"/><img src="/images/posts/2018-02-05-generative-modeling-and-ai/displacement_graphs_b_1.png" height=256 alt="Точность"/></center>
<center>Рис. 3. Графики смещения от начала координат (параметра $b$).</center>

Видно, что $b_i$ быстро сошлись к аналитическим значениям $5$ и $10$. Давайте теперь посмотрим на параметры $W, \sigma$:

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/graph_sigma.png" description="Рис. 4. График изменения параметра $\sigma$." width=1024 %} </center>

Видно, что значение $\sigma$ сошлось к $1.2$, т.е. к меньшей оси отклонения входного распределения, как и ожидалось.

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/weight_0.png" width=1024 %} </center>
<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/weight_1.png" description="Рис. 5. Графики изменения параметров $W\_{i0}$." width=1024 %} </center>

$W$, в свою очередь, сошлось к примерно такому значению, при котором $W W^T + \sigma^2 I=\\begin{pmatrix} 1.2^2 & 0 \\\\ 0 & 2.4^2 \\end{pmatrix}$. Подставляя эти значения в модель, мы получаем $$P(x)=N(x; b, W W^T + \sigma^2 I)=N(x; \\begin{pmatrix} 5 \\\\ 10 \\end{pmatrix}, \\begin{pmatrix} 1.2^2 & 0 \\\\ 0 & 2.4^2 \\end{pmatrix})$$, что значит, что мы восстановили распределение данных.

Давайте посмотрим на распределения данных. Латентная переменная одномерна, потому она отображена как одномерное распределение. Видимая переменная же двумерна, но ее заданная матрица ковариации диагональна, что значит, что ее эллипсоид выровнен с осями координат. Потому мы отобразим ее как две проекции ее распределения на оси координат. Так выглядят заданное и выученное распределение $P(x)$ в проекции на первую ось координат:

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/sample_real_0.png" description="Рис. 6. Проекция заданного распределения $P(x)$ на первую ось координат." width=1024 %} </center>

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/sample_learnerd_0.png" description="Рис. 7. Проекция выученного распределения $P(x|\theta)$ на первую ось координат." width=900 %} </center>

А так выглядят заданное и выученное распределение $P(x)$ в проекции на вторую ось координат:

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/sample_real_1.png" description="Рис. 8. Проекция заданного распределения $P(x)$ на вторую ось координат.
" width=900 %} </center>
<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/sample_learnerd_1.png" description="Рис. 9. Проекция выученного распределения $P(x|\theta)$ на вторую ось координат." width=900 %} </center>

А так выглядят аналитическое и выученное распределения $P(z)$:

<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/latent_sample_real.png" description="Рис. 10. Заданное распределение $P(z)=N(z; 0, 1)$." width=900 %} </center>
<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/latent_sample_learned.png" description="Рис. 11. Выученное распределение $P(z|\theta)$." width=900 %} </center>

Видно, что все выученные распределения сходятся к распределениям, очень похожим на заданные задачей. Давайте посмотрим на динамику обучения модели, чтобы убедиться в этом окончательно:
<center> {% include image_with_caption.html url="/images/posts/2018-02-05-generative-modeling-and-ai/learning_process.gif" description="Рис. 12. Процесс обучения модели PPCA, при котором выученное распределение $P(x|\theta)$ сходится к распределению данных $P(x)$." width=520 %} </center>

## Заключение

Приведенная модель &mdash; это вероятностная интерпретация классической модели PCA, которая является линейной моделью. Мы использовали математику из [оригинальной статьи](http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf), построили GEM алгоритм поверх нее и показали, что получившаяся модель сходится к аналитическому решению на простом примере. Разумеется, если бы в задаче $P(x)$ не было нормальным, модель бы не справилась так хорошо. Точно так же, как PCA не справляется идеально с данными, не лежащими в некой гиперплоскости. Для решения более сложных задач аппроксимации распределений нам понадобятся более сложные и нелинейные модели. Об одной из таких моделей, Generative Adversarial Networks, и пойдет речь в следующей статье.

## Заключение

Спасибо [Olga Talanova](https://www.linkedin.com/in/olga-talanova-b319b761/) за ревью текста. Спасибо [Andrei Tarashkevich](https://github.com/andrewtar) за помощь с версткой этой статьи.
